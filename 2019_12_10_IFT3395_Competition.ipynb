{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train_data = np.load('data_train.pkl', allow_pickle=True)\n",
    "train_data = np.array(train_data)\n",
    "train_data = train_data.T\n",
    "\n",
    "test_data = np.load('data_test.pkl', allow_pickle=True)\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\draby\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# text preprocessing\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer \n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "# function to lemmatize sentences\n",
    "def lemmatize_sentence(sentence):\n",
    "    # tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    # tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            # if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            # else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = tokenizer.tokenize(text)\n",
    "\n",
    "    # make all text lowercase\n",
    "    text = np.char.lower(text)\n",
    "    \n",
    "    # remove all non-alphabetic terms\n",
    "    text = [word for word in text if word.isalpha()]\n",
    "    \n",
    "    # remove all stopwords\n",
    "    text = ' '.join(word for word in text if word not in STOPWORDS)\n",
    "    \n",
    "    # lemmatize words\n",
    "    text = lemmatize_sentence(text)\n",
    "    \n",
    "    # stemming\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "train_data[:,0] = np.array([clean_text(ex) for ex in train_data[:,0]])\n",
    "test_data = np.array([clean_text(ex) for ex in test_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAG OF WORDS\n",
    "\n",
    "# create a dictionary of word frequencies\n",
    "word_dict = {}\n",
    "for text in train_data[:,0]:\n",
    "    tokens = text.split(' ')\n",
    "    for token in tokens:\n",
    "        if token not in word_dict.keys():\n",
    "            word_dict[token] = 1\n",
    "        else:\n",
    "            word_dict[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only the words that have a frequency greater than 1\n",
    "word_dict_reduced = dict(filter(lambda elem: elem[1] > 1, word_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24833"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of word frequencies for each example\n",
    "def vectorize(text):\n",
    "    words = text.split(' ')\n",
    "    words = np.array(words)\n",
    "    sentence_vector = []\n",
    "    for term in word_dict_reduced:\n",
    "        sentence_vector.append(np.sum(words == term))\n",
    "    \n",
    "    # normalize the frequencies\n",
    "    sentence_vector = np.array(sentence_vector)\n",
    "    \n",
    "    if sentence_vector.max() > 0:\n",
    "        sentence_vector = sentence_vector / sentence_vector.max()\n",
    "    \n",
    "    return sentence_vector\n",
    "    \n",
    "    \n",
    "X = np.array([vectorize(ex) for ex in train_data[:,0]])\n",
    "y = train_data[:,1]\n",
    "test_data = np.array([vectorize(ex) for ex in test_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ______________NAIVE BAYES____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split our training and validation data\n",
    "import random\n",
    "\n",
    "def data_split(X, y, ratio):\n",
    "    # set seed\n",
    "    random.seed(3395)\n",
    "    \n",
    "    # choose indexes for the train and validation data\n",
    "    inds = list(range(X.shape[0]))\n",
    "    random.shuffle(inds)\n",
    "    train_inds = inds[:int(X.shape[0] * ratio)]\n",
    "    validation_inds = inds[int(X.shape[0] * ratio):]\n",
    "    \n",
    "    # split the data into both sets\n",
    "    train_X = X[train_inds, :]\n",
    "    validation_X = X[validation_inds, :]\n",
    "    train_y = y[train_inds]\n",
    "    validation_y = y[validation_inds]\n",
    "    \n",
    "    return train_X, validation_X, train_y, validation_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceMaxLikelihood:\n",
    "    \n",
    "    def __init__(self, n_dims):\n",
    "        self.log_likelihood = np.zeros(n_dims)\n",
    "\n",
    "    def train(self, train_data):\n",
    "        word_frequencies = np.sum(train_data, axis=0) + 0.1\n",
    "        self.log_likelihood = np.log(word_frequencies / np.sum(word_frequencies))    \n",
    "\n",
    "    def loglikelihood(self, test_data):\n",
    "        log_prob = np.dot(test_data, self.log_likelihood)\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesClassifier:\n",
    "\n",
    "    def __init__(self,models_ml, priors):\n",
    "        self.models_ml = models_ml\n",
    "        self.priors = priors\n",
    "        if len(self.models_ml) != len(self.priors):\n",
    "            print('The number of ML models must be equal to the number of priors!')\n",
    "        self.n_classes = len(self.models_ml)\n",
    "        \n",
    "    def loglikelihood(self, test_data, eval_by_group=False):\n",
    "        log_pred = np.empty((test_data.shape[0],self.n_classes))\n",
    "        \n",
    "        for i in range(self.n_classes):\n",
    "            log_pred[:,i] = self.models_ml[i].loglikelihood(test_data) +  np.log(self.priors[i])\n",
    "\n",
    "        return log_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model for each class using its maximum likelihood and train the corresponding data\n",
    "labels = np.unique(y)\n",
    "\n",
    "models = []\n",
    "priors = []\n",
    "\n",
    "train_X = []\n",
    "train_y = []\n",
    "validation_X = []\n",
    "validation_y = []\n",
    "\n",
    "for label in labels:\n",
    "    X_class = X[y == label]\n",
    "    y_class = y[y == label]\n",
    "    class_model = LaplaceMaxLikelihood(X_class.shape[1]) #choose Estimator\n",
    "    \n",
    "    train_X_class, validation_X_class, train_y_class, validation_y_class = data_split(X_class, y_class, 0.7)\n",
    "    \n",
    "    if len(train_X) == 0:\n",
    "        train_X = np.array(train_X_class)\n",
    "        train_y = np.array(train_y_class)\n",
    "        validation_X = np.array(validation_X_class)\n",
    "        validation_y = np.array(validation_y_class)\n",
    "    else:\n",
    "        train_X = np.vstack((train_X, np.array(train_X_class)))\n",
    "        train_y = np.hstack((train_y, np.array(train_y_class)))\n",
    "        validation_X = np.vstack((validation_X, np.array(validation_X_class)))\n",
    "        validation_y = np.hstack((validation_y, np.array(validation_y_class)))\n",
    "    \n",
    "    class_model.train(train_X_class)\n",
    "    models.append(class_model)\n",
    "    priors.append(X_class.shape[0] / X.shape[0])\n",
    "    \n",
    "    \n",
    "# create our classifier using our class models and priors\n",
    "classifier = BayesClassifier(models, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is : 75.9 % \n",
      "The validation accuracy is : 55.5 % \n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(test_inputs, test_labels):\n",
    "    log_prob = classifier.loglikelihood(test_inputs)\n",
    "    classes_pred = labels[log_prob.argmax(1)]\n",
    "    return np.mean(classes_pred == test_labels)\n",
    "\n",
    "\n",
    "print(\"The training accuracy is : {:.1f} % \".format(100 * get_accuracy(train_X, train_y)))\n",
    "print(\"The validation accuracy is : {:.1f} % \".format(100 * get_accuracy(validation_X, validation_y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for the test data\n",
    "\n",
    "log_prob = classifier.loglikelihood(test_data)\n",
    "classes_pred = labels[log_prob.argmax(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export predictions to csv file\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('test.csv', 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow(['Id', 'Category'])\n",
    "    i=0\n",
    "    for word in classes_pred:\n",
    "        wr.writerow([i, word])\n",
    "        i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
